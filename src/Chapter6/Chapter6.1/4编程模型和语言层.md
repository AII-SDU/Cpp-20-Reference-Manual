# 编程模型和语言层
##### TPU-MLIR 介绍

TPU-MLIR 是算能深度学习处理器的编译器工程，提供了一套完整的工具链，用于将不同框架下预训练的神经网络转化为可以在算能智能视觉深度学习处理器上高效运行的模型文件（BModel/CviModel）。

##### 主要特点

1. 支持多种框架：直接支持 PyTorch、ONNX、TFLite 和 Caffe 等框架模型的转换。
2. 开源：代码已开源到 GitHub（https://github.com/sophgo/tpu-mlir）。
3. 学术支持：有相关论文描述其整体设计思路（https://arxiv.org/abs/2210.15016）。

##### 架构概览

TPU-MLIR 的整体架构包括以下主要组件：

1. 前端转换：将各种框架的模型转换为 MLIR 表示。
2. 优化阶段：对 MLIR 进行各种优化。
3. 后端生成：生成可在 TPU 上运行的 BModel/CviModel。

#####  使用流程

###### 1. 模型转换

使用 `model_transform` 工具将原始模型转换成 MLIR 文件。

###### 2. 量化（可选）

如需 INT8 量化：
- 使用 `run_calibration` 生成校准表。
- 使用 `run_qtable` 生成量化表（用于决定哪些层采用浮点计算）。

###### 3. 模型部署

使用 `model_deploy` 将 MLIR 文件转换成 BModel/CviModel。

##### Lowering 过程
TPU-MLIR使用两种主要的方言：TOP（Tensor Operator）和TPU。

- TOP方言：
  - 硬件无关层
  - 支持F32/F16/BF16/INT8对称/INT8非对称等类型
  - 代表了网络的高级表示

- TPU方言：
  - 硬件相关层
  - 针对特定TPU硬件优化
  - 包含了硬件特定的量化和优化策略



Lowering是将TOP层OP下沉到TPU层OP的过程：
- 将算子从硬件无关层(TOP)转换到硬件相关层(TPU)
- 支持F32/F16/BF16/INT8对称/INT8非对称等类型转换
- 涉及量化算法，针对不同硬件有不同实现
- 处理混合精度情况，在需要时插入CastOp
Lowering 是将 TOP 层 OP 下沉到 TPU 层 OP 的过程：

- 支持 F32/F16/BF16/INT8 对称/INT8 非对称等类型转换
- 处理混合精度情况，在需要时插入 CastOp

##### CodeGen 过程

CodeGen 是将 MLIR 文件转换为最终 BModel 的过程，主要包括：

1. 指令生成：执行不同 op 的 CodeGen 接口，生成相应的二进制指令
2. 指令存储：使用 store_cmd 将指令存储在指定数据结构中
3. 指令取出：所有 op 的二进制码生成完毕后，调用 BM168X 系列类中封装的函数取出指令，最终生成 BModel

##### 后端实现

- 使用动态库（libbackend_xxx.so）封装硬件后端
- 通过函数指针加载后端函数
- 使用 EngineStorer 和 CmdStorer 系列类管理指令存储
- 采用单例模式和装饰器模式实现灵活的指令管理

通过这种设计，TPU-MLIR 能够有效地将各种深度学习框架的模型转换为可在 TPU 上高效运行的 BModel，同时提供了灵活的优化和定制空间。




基于提供的上下文，我可以为您综合一个关于TPU-MLIR的介绍，包括TOP和TPU两种方言，以及CodeGen的过程：

##### 自定义算子开发

TPU-MLIR 支持添加自定义算子，主要步骤如下：

###### 1. 前端定义

使用 TpuLang 接口定义自定义算子：

```python
import transform.TpuLang as tpul

tpul.init("BM1684X", True)

# 定义输入
x = tpul.Tensor(dtype="float32", shape=[1, 3, 224, 224], name="input")

# 添加自定义算子
def shape_func(tensors_in):
    return [tensors_in[0].shape]

outs = tpul.custom(
    tensors_in=[x],
    shape_func=shape_func,
    op_name="custom_op_name",
    params={"param1": value1, "param2": value2},
    out_dtypes=["float32"],
    out_names=["custom_out"]
)

# 编译生成 MLIR
tpul.compile("model_name", [x], outs, False, 2, has_custom=True)
```

###### 2. 后端实现

1. 在 `$TPUC_ROOT/customlayer/include` 添加头文件。
2. 在 `$TPUC_ROOT/customlayer/src` 添加实现文件。
3. 在 `backend_custom_param.h` 定义参数结构体。
4. 添加 API 接口文件。
5. 在 `backend_custom_api.cpp` 定义后端调用接口。
6. 运行 `$TPUC_ROOT/customlayer/build.sh` 编译生成动态库。


TPU-MLIR 提供了一个强大的工具链，支持从多种深度学习框架到 TPU 可执行模型的转换。通过支持自定义算子，它为开发者提供了极大的灵活性，使得复杂的深度学习模型能够在 TPU 上高效运行。结合 TPUPerf 工具，开发者可以全面优化和验证其模型性能。
