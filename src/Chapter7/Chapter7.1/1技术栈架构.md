# 技术栈架构

MUSA (Metaverse Unified System Architecture)，是摩尔线程(Moore Threads)推出的运算平台。MUSA 是一种通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 它包含了 MUSA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。

**1. 系统软件层**

- **摩尔线程GPU驱动**：提供GPU的基本系统级支持
- **MUSA Driver API**：低级 API，提供对 GPU 的直接控制
  - 开发者需要做显式的设备初始化、context管理和module管理
  - 允许直接管理设备、内存分配和程序执行
  - 适用于需要细粒度控制的高级应用

**2. 运行时环境层**

- **MUSA Runtime API**：基于MUSA Driver API的封装
  - 提供更高级的抽象，简化了 GPU 的使用
  - 隐式完成了设备初始化，context管理和module管理
  - 更适合一般开发者使用，提供了更好的易用性

**3. 编程模型和语言层**

- **MUSA C/C++**：是一种专为摩尔线程GPU设计的编程语言，它是C++的扩展，允许开发者编写可在GPU上执行的程序。
  - MUSA C++由C++语言扩展和运行时库组成。
  - 核心C++语言扩展引入了编程模型。
  - 运行时库提供在Host上执行的函数。

**4. 计算库层**

- **muBLAS**：提供基础线性代数运算，优化了AI和高性能计算(HPC)场景。
- **muFFT**：执行快速傅里叶变换。
- **muRAND**：生成伪随机数和准随机数。
- **muSPARSE**：专注于稀疏矩阵的数学运算，优化存储和计算效率。
- **muPP**：提供图像和信号处理的高性能函数库。

- **MCCL**：摩尔线程集合通信库，支持多GPU和多节点通信，提供高效的数据传输和同步机制。
  - MCCL 提供了 all-gather、all-reduce、broadcast、reduce、reduce-scatter、point-to-point send 和 receive 等原语，可通过节点内的 PCIe 和 MTLink 高速互联以及节点间的InfiniBand网络实现高带宽和低延迟。
  - MCCL支持节点内和跨节点通信。可以实现拓扑的自动检测，计算最佳的路径，最终实现GPUs之间的高效传输
- **muDNN**：专为深度学习设计的加速库，支持多种网络模型和算子。
  - 支持多种深度学习模型，如CNN、RNN、GNN等。
  - 提供了高度优化的算子实现，如卷积、池化、激活函数等。
  - 支持自动并行计算和内存管理，提高计算效率。
- **muTENSOR**：提供张量计算支持，优化了机器学习和科学计算的性能。
  - 提供了丰富的张量操作，如张量创建、切片、合并、数学运算等。
  - 高度优化的张量计算性能，支持并行计算和GPU加速。
  - 支持多种数据类型和设备，如GPU、CPU等。
- **muTLASS**：图计算库，支持图神经网络(GNN)的高效运算。
  - 支持多种图神经网络模型，如GCN、GAT、GraphSAGE等。
  - 提供了高度优化的图计算算子，如消息传递、读取邻居等。
  - 支持自动并行计算和内存管理，提高计算效率。
- **MT Data Loader**：高效的数据加载库，支持并行数据加载和预处理。
  - 支持多种数据格式，如图片、文本、音频等。
  - 提供了高速的数据加载和预处理能力，支持并行加载和多线程处理。
  - 支持在线数据增强和预处理，减少数据加载时间。
**5. 模型框架层**
- **Torch MUSA**：摩尔线程适配了流行的开源AI框架PyTorch，使得PyTorch能够利用MUSA GPU进行加速计算，极大地提升了深度学习模型的开发和部署效率。
  - CUDA 兼容性：torch_musa 可以实现与 CUDA 的兼容，这大大减少了适配新操作符的工作量。
  - API 一致性：torch_musa 的 API 格式与 PyTorch 一致，使习惯使用 PyTorch 的用户能够顺利迁移到 torch_musa。
